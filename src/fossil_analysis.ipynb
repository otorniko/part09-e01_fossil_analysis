{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting patterns of speciation in the fos- sil record\n",
    "\n",
    "In this assignment, we use data from the NOW (New and Old Worlds) database of fossil mammals to study patterns of speciation over time and space. In particular, we are interested to know when and where speciation rates have been significantly high. The task is to find which time periods and which places over the history of mammals have given rise to exceptionally high numbers of new species. The phenomenon is known in the evolutionary literature as the “species factory”. Palaeontologists are interested why and in which ways those times and places are special. The role of computational science is to identify and characterize such times and places.\n",
    "We practice using pandas DataFrames, performing logistic regression and making statistical significance tests in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. Create a pandas DataFrame that contains all of the data\n",
    "and save it as a csv file. How many rows does the DataFrame contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"now.txt\", sep=',')\n",
    "\n",
    "num_rows = len(df)\n",
    "print(f\"The DataFrame contains {num_rows} rows.\")\n",
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3. a) Remove all rows where LAT = LONG = 0; these occurrences have incorrect coordinates. Drop rows where SPECIES is “sp.” or\n",
    "“indet.”; these occurrences have not been properly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_lat_long_zero = (df['LAT'] == 0) & (df['LONG'] == 0)\n",
    "df = df[~condition_lat_long_zero]\n",
    "        \n",
    "print(f\"Number of rows after removing LAT=0 & LONG=0: {len(df)}\")\n",
    "\n",
    "species_to_remove = [\"sp.\", \"indet.\"]\n",
    "condition_unidentified_species = df['SPECIES'].isin(species_to_remove)\n",
    "df = df[~condition_unidentified_species]\n",
    "\n",
    "print(f\"Number of rows after removing unidentified species ('sp.', 'indet.'): {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Next we will assign each occurrence to a specific Mammal Neogene\n",
    "(MN) time unit. Table 1 shows the time boundaries of each time unit.\n",
    "Assign each occurrence to a correct time unit by calculating the mean of\n",
    "MIN AGE and MAX AGE. If the mean age of an occurrence is precisely\n",
    "on the boundary between two time units, assign the occurrence to the\n",
    "older time unit. If the mean age of an occurrence is outside of the MN\n",
    "time interval, assign it to a “pre-MN” or “post-MN” category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_boundaries = {\n",
    "    \"MN1\": (23.0, 21.7),\n",
    "    \"MN2\": (21.7, 19.5),\n",
    "    \"MN3\": (19.5, 17.2),\n",
    "    \"MN4\": (17.2, 16.4),\n",
    "    \"MN5\": (16.4, 14.2),\n",
    "    \"MN6\": (14.2, 12.85),\n",
    "    \"MN7-8\": (12.85, 11.2),\n",
    "    \"MN9\": (11.2, 9.9),\n",
    "    \"MN10\": (9.9, 8.9),\n",
    "    \"MN11\": (8.9, 7.6),\n",
    "    \"MN12\": (7.6, 7.1),\n",
    "    \"MN13\": (7.1, 5.3),\n",
    "    \"MN14\": (5.3, 5.0),\n",
    "    \"MN15\": (5.0, 3.55),\n",
    "    \"MN16\": (3.55, 2.5),\n",
    "    \"MN17\": (2.5, 1.9),\n",
    "    \"MQ18\": (1.9, 0.85), \n",
    "    \"MQ19\": (0.85, 0.01) \n",
    "}\n",
    "\n",
    "overall_start_age = mn_boundaries[\"MN1\"][0]\n",
    "overall_end_age = mn_boundaries[\"MQ19\"][1]\n",
    "\n",
    "\"\"\" Calculate mean age, handling potential NaN values if necessary\n",
    "    Assigns an occurrence to an MN unit based on its mean age.\"\"\"\n",
    "def assign_mn_unit(row):\n",
    "    if pd.isna(row['MIN AGE']) or pd.isna(row['MAX AGE']):\n",
    "        return 'unknown_age' \n",
    "    mean_age = (row['MIN AGE'] + row['MAX AGE']) / 2.0\n",
    "    if mean_age > overall_start_age:\n",
    "        return 'pre-MN'\n",
    "    elif mean_age < overall_end_age: \n",
    "        return 'post-MN'\n",
    "    else:\n",
    "        for unit, (start_age, end_age) in mn_boundaries.items():\n",
    "            if mean_age >= end_age and mean_age < start_age:\n",
    "                return unit\n",
    "        if mean_age == overall_end_age:\n",
    "             for unit, (start_age, end_age) in mn_boundaries.items():\n",
    "                 if end_age == overall_end_age:\n",
    "                     return unit\n",
    "    return 'assignment_error' \n",
    "df['MIN AGE'] = pd.to_numeric(df['MIN AGE'], errors='coerce')\n",
    "df['MAX AGE'] = pd.to_numeric(df['MAX AGE'], errors='coerce')\n",
    "df['MN_Unit'] = df.apply(assign_mn_unit, axis=1)\n",
    "print(\"First 5 rows with assigned MN Unit:\")\n",
    "print(df[['LIDNUM', 'MIN AGE', 'MAX AGE', 'MN_Unit']].head())\n",
    "print(\"\\nCounts per MN Unit:\")\n",
    "print(df['MN_Unit'].value_counts().sort_index())\n",
    "print(\"\\nChecking for assignment issues:\")\n",
    "print(df[df['MN_Unit'] == 'unknown_age'][['LIDNUM', 'MIN AGE', 'MAX AGE']].head())\n",
    "print(df[df['MN_Unit'] == 'assignment_error'][['LIDNUM', 'MIN AGE', 'MAX AGE']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Sometimes expert knowledge may be used to override some of the\n",
    "information recorded in the data. In our case, experts in palaeontology\n",
    "tell us that occurrences in the localities “Samos Main Bone Beds” and\n",
    "“Can Llobateres I” should be assigned to time units MN12 and MN9,\n",
    "respectively. Check these and if necessary, edit the time units to their\n",
    "correct values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the specified localities exist in the dataset\n",
    "samos = df[df['NAME'] == 'Samos Main Bone Beds']\n",
    "can_llobateres = df[df['NAME'] == 'Can Llobateres I']\n",
    "\n",
    "# Print current information\n",
    "print(\"Before correction:\")\n",
    "print(f\"Samos Main Bone Beds: {samos['MN_Unit'].unique() if len(samos) > 0 else 'Not found'}\")\n",
    "print(f\"Can Llobateres I: {can_llobateres['MN_Unit'].unique() if len(can_llobateres) > 0 else 'Not found'}\")\n",
    "\n",
    "# Update MN units based on expert knowledge\n",
    "df.loc[df['NAME'] == 'Samos Main Bone Beds', 'MN_Unit'] = 'MN12'\n",
    "df.loc[df['NAME'] == 'Can Llobateres I', 'MN_Unit'] = 'MN9'\n",
    "\n",
    "# Verify the changes\n",
    "samos = df[df['NAME'] == 'Samos Main Bone Beds']\n",
    "can_llobateres = df[df['NAME'] == 'Can Llobateres I']\n",
    "\n",
    "print(\"\\nAfter correction:\")\n",
    "print(f\"Samos Main Bone Beds: {samos['MN_Unit'].unique() if len(samos) > 0 else 'Not found'}\")\n",
    "print(f\"Can Llobateres I: {can_llobateres['MN_Unit'].unique() if len(can_llobateres) > 0 else 'Not found'}\")\n",
    "print(f\"Number of records updated for Samos Main Bone Beds: {len(samos)}\")\n",
    "print(f\"Number of records updated for Can Llobateres I: {len(can_llobateres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) We need to be able to identify all occurrences of each species. Assign a unique identification number for each unique combination of GENUS and SPECIES. Create a new column in the DataFrame and label each\n",
    "occurrence with a corresponding species identification number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create a unique species identifier for each GENUS-SPECIES combination\n",
    "# First create a concatenated field\n",
    "df['full_species'] = df['GENUS'] + ' ' + df['SPECIES']\n",
    "\n",
    "# Get unique combinations and create a mapping dictionary\n",
    "unique_species = df['full_species'].unique()\n",
    "species_id_map = {species: idx for idx, species in enumerate(unique_species, 1)}\n",
    "\n",
    "# Assign IDs to each occurrence\n",
    "df['species_id'] = df['full_species'].map(species_id_map)\n",
    "\n",
    "# Display information about the species IDs\n",
    "print(f\"Total number of unique species: {len(unique_species)}\")\n",
    "print(\"\\nFirst 10 species with their IDs:\")\n",
    "print(df[['GENUS', 'SPECIES', 'full_species', 'species_id']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Each locality should contain no more than one occurrence of any\n",
    "species. Check whether this is the case and remove duplicate copies, if\n",
    "necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates (same species in same locality)\n",
    "duplicate_check = df.groupby(['LIDNUM', 'species_id']).size().reset_index(name='count')\n",
    "duplicates = duplicate_check[duplicate_check['count'] > 1]\n",
    "\n",
    "# Print information about duplicates\n",
    "print(f\"Number of locality-species combinations with duplicates: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nExample of duplicates:\")\n",
    "    print(duplicates.head())\n",
    "    \n",
    "    # Get an example of duplicate records\n",
    "    if not duplicates.empty:\n",
    "        first_duplicate = duplicates.iloc[0]\n",
    "        lid = first_duplicate['LIDNUM']\n",
    "        sp_id = first_duplicate['species_id']\n",
    "        \n",
    "        print(f\"\\nDetailed view of a duplicate example (LIDNUM={lid}, species_id={sp_id}):\")\n",
    "        print(df[(df['LIDNUM'] == lid) & (df['species_id'] == sp_id)].head())\n",
    "    \n",
    "    # Remove duplicates - keep only the first occurrence of each species in each locality\n",
    "    original_size = len(df)\n",
    "    df = df.drop_duplicates(subset=['LIDNUM', 'species_id'])\n",
    "    \n",
    "    print(f\"\\nOriginal DataFrame size: {original_size}\")\n",
    "    print(f\"After removing duplicates: {len(df)}\")\n",
    "    print(f\"Number of duplicates removed: {original_size - len(df)}\")\n",
    "else:\n",
    "    print(\"No duplicates found. Each species appears only once per locality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) How many rows are we left with in the DataFrame (compare with\n",
    "exercise 2)? How many unique species and localities are identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current number of rows\n",
    "current_rows = len(df)\n",
    "print(f\"Current number of rows in DataFrame: {current_rows}\")\n",
    "\n",
    "# Original number of rows from Exercise 2\n",
    "print(f\"Original number of rows (from Exercise 2): {num_rows}\")\n",
    "print(f\"Difference: {num_rows - current_rows} rows were removed\")\n",
    "\n",
    "# Count unique species\n",
    "unique_species_count = df['species_id'].nunique()\n",
    "print(f\"Number of unique species: {unique_species_count}\")\n",
    "\n",
    "# Count unique localities\n",
    "unique_localities_count = df['LIDNUM'].nunique()\n",
    "print(f\"Number of unique localities: {unique_localities_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4. Create a DataFrame that shows for each species how many\n",
    "occurrences it has in each time unit. Then, create a different DataFrame\n",
    "that shows for each species the time unit when it is first observed (i.e.\n",
    "the oldest time unit). For each time unit, calculate the proportion of first\n",
    "occurrences to all occurrences. Plot the proportion of first occurrences\n",
    "over time. Also, plot the total number of occurrences over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: DataFrame showing occurrences per species per time unit\n",
    "species_time_occurrences = pd.crosstab(df['species_id'], df['MN_Unit'])\n",
    "\n",
    "# Step 2: Find the first (oldest) time unit for each species\n",
    "# Create a dictionary to map time units to their chronological order\n",
    "mn_order = {\n",
    "    \"pre-MN\": 0,\n",
    "    \"MN1\": 1, \"MN2\": 2, \"MN3\": 3, \"MN4\": 4, \"MN5\": 5,\n",
    "    \"MN6\": 6, \"MN7-8\": 7, \"MN9\": 8, \"MN10\": 9, \"MN11\": 10,\n",
    "    \"MN12\": 11, \"MN13\": 12, \"MN14\": 13, \"MN15\": 14,\n",
    "    \"MN16\": 15, \"MN17\": 16, \"MQ18\": 17, \"MQ19\": 18,\n",
    "    \"post-MN\": 19\n",
    "}\n",
    "\n",
    "# Add chronological order column to assist in finding first occurrence\n",
    "df['MN_Order'] = df['MN_Unit'].map(mn_order)\n",
    "\n",
    "# Find first occurrence time unit for each species\n",
    "first_occurrences = df.loc[df.groupby('species_id')['MN_Order'].idxmin()]\n",
    "first_occ_df = pd.DataFrame({\n",
    "    'species_id': first_occurrences['species_id'],\n",
    "    'first_MN_Unit': first_occurrences['MN_Unit']\n",
    "})\n",
    "\n",
    "# Step 3: Calculate counts and proportions per time unit\n",
    "total_occurrences = df['MN_Unit'].value_counts().sort_index()\n",
    "first_occ_counts = first_occurrences['MN_Unit'].value_counts().sort_index()\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results = pd.DataFrame({\n",
    "    'total_occurrences': total_occurrences,\n",
    "    'first_occurrences': first_occ_counts.reindex(total_occurrences.index, fill_value=0)\n",
    "})\n",
    "\n",
    "# Calculate proportions\n",
    "results['proportion'] = results['first_occurrences'] / results['total_occurrences']\n",
    "\n",
    "# Step 4 & 5: Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out pre-MN and post-MN for better visualization\n",
    "plot_results = results[~results.index.isin(['pre-MN', 'post-MN', 'unknown_age', 'assignment_error'])]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Proportion of first occurrences\n",
    "ax1.bar(plot_results.index, plot_results['proportion'])\n",
    "ax1.set_xlabel('Time Unit')\n",
    "ax1.set_ylabel('Proportion of First Occurrences')\n",
    "ax1.set_title('Proportion of First Occurrences by Time Unit')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45)\n",
    "\n",
    "# Plot 2: Total occurrences\n",
    "ax2.bar(plot_results.index, plot_results['total_occurrences'])\n",
    "ax2.set_xlabel('Time Unit')\n",
    "ax2.set_ylabel('Number of Occurrences')\n",
    "ax2.set_title('Total Occurrences by Time Unit')\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.setp(ax2.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of the results\n",
    "print(\"Summary of occurrences and first appearances by time unit:\")\n",
    "print(results[['total_occurrences', 'first_occurrences', 'proportion']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5. a) Create a DataFrame that collects the following information for every locality: locality number (LIDNUM), longitude, latitude,\n",
    "time unit, number of first occurrences in the locality, number of all occurrences in the locality and proportion of first occurrences in the locality. Note, you should use LIDNUM to identify unique localities and not the NAME variable (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the required locality information\n",
    "# 1. Get locality information (lat, long, time unit)\n",
    "locality_info = df.groupby('LIDNUM').first().reset_index()\n",
    "locality_df = locality_info[['LIDNUM', 'LONG', 'LAT', 'MN_Unit']]\n",
    "\n",
    "# 2. Count all occurrences per locality\n",
    "all_occurrences = df.groupby('LIDNUM').size().reset_index(name='total_occurrences')\n",
    "\n",
    "# 3. Count first occurrences per locality\n",
    "# Get species that are first occurrences\n",
    "species_first_occ = df.loc[df.groupby('species_id')['MN_Order'].idxmin(), ['species_id', 'LIDNUM']]\n",
    "first_occ_count = species_first_occ.groupby('LIDNUM').size().reset_index(name='first_occurrences')\n",
    "\n",
    "# 4. Merge the information\n",
    "locality_df = locality_df.merge(all_occurrences, on='LIDNUM', how='left')\n",
    "locality_df = locality_df.merge(first_occ_count, on='LIDNUM', how='left')\n",
    "\n",
    "# 5. Fill NAs with 0 (localities with no first occurrences)\n",
    "locality_df['first_occurrences'] = locality_df['first_occurrences'].fillna(0)\n",
    "\n",
    "# 6. Calculate proportion of first occurrences\n",
    "locality_df['proportion_first'] = locality_df['first_occurrences'] / locality_df['total_occurrences']\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total number of localities: {len(locality_df)}\")\n",
    "print(\"\\nFirst 5 localities:\")\n",
    "print(locality_df.head())\n",
    "\n",
    "# Answer to why use LIDNUM instead of NAME:\n",
    "print(\"\\nWhy use LIDNUM instead of NAME?\")\n",
    "print(\"- LIDNUM is a unique identifier for each locality\")\n",
    "print(\"- NAME might not be unique (different localities could have the same name)\")\n",
    "print(\"- NAME could have variations in spelling, formatting, or data entry errors\")\n",
    "print(\"- Multiple localities might be part of the same named location but have distinct coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Visualize the distribution of localities in space and time. For each time\n",
    "unit, plot the LAT and LONG coordinates of each locality (corresponding\n",
    "to the time unit). For example, you can use the above codes to create a\n",
    "geographic map and then use a standard matplotlib scatter plot to add\n",
    "the localities. Choose the marker size for each locality such that it is\n",
    "relative to the number of occurrences in the locality (bigger markers for\n",
    "bigger localities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include standard MN time units\n",
    "valid_mn_units = [\n",
    "    \"MN1\", \"MN2\", \"MN3\", \"MN4\", \"MN5\", \"MN6\", \"MN7-8\", \"MN9\", \"MN10\", \n",
    "    \"MN11\", \"MN12\", \"MN13\", \"MN14\", \"MN15\", \"MN16\", \"MN17\", \"MQ18\", \"MQ19\"\n",
    "]\n",
    "\n",
    "# Filter out localities with invalid MN_Units\n",
    "filtered_localities = locality_df[locality_df['MN_Unit'].isin(valid_mn_units)].copy()\n",
    "\n",
    "# Set up the grid of plots\n",
    "n_cols = 4  # Number of columns in the plot grid\n",
    "n_rows = (len(valid_mn_units) + n_cols - 1) // n_cols  # Calculate number of rows needed\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5 * n_rows))\n",
    "\n",
    "# Get min and max occurrences for consistent scaling across all plots\n",
    "overall_min_occurrences = filtered_localities['total_occurrences'].min()\n",
    "overall_max_occurrences = filtered_localities['total_occurrences'].max()\n",
    "\n",
    "# Create a common normalization for all plots (for consistent colors)\n",
    "norm = Normalize(vmin=overall_min_occurrences, vmax=overall_max_occurrences)\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# Plot each time unit\n",
    "for i, unit in enumerate(valid_mn_units):\n",
    "    # Get data for this time unit\n",
    "    unit_data = filtered_localities[filtered_localities['MN_Unit'] == unit]\n",
    "    \n",
    "    if len(unit_data) > 0:  # Only create plot if there are localities for this time unit\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # Scale marker sizes based on number of occurrences (min 20, max 200)\n",
    "        sizes = 20 + 180 * (unit_data['total_occurrences'] - overall_min_occurrences) / (overall_max_occurrences - overall_min_occurrences)\n",
    "        sizes = np.maximum(sizes, 20)  # Ensure minimum size\n",
    "        \n",
    "        # Plot localities as scatter points\n",
    "        scatter = ax.scatter(\n",
    "            unit_data['LONG'], \n",
    "            unit_data['LAT'], \n",
    "            s=sizes,  # Size based on number of occurrences\n",
    "            c=unit_data['total_occurrences'],  # Color based on number of occurrences\n",
    "            cmap=cmap, \n",
    "            norm=norm,\n",
    "            alpha=0.7, \n",
    "            edgecolor='black'\n",
    "        )\n",
    "        \n",
    "        # Set plot limits to focus on the Europe-Asia-Africa area\n",
    "        ax.set_xlim(-20, 60)\n",
    "        ax.set_ylim(0, 60)\n",
    "        \n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title(f\"{unit} ({len(unit_data)} localities)\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add a colorbar to show the scale\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])  # [left, bottom, width, height]\n",
    "sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.set_label('Number of Occurrences')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to make room for colorbar\n",
    "plt.suptitle('Distribution of Fossil Mammal Localities by Time Unit', fontsize=16, y=0.92)\n",
    "plt.show()\n",
    "\n",
    "print(\"The visualization shows how fossil mammal localities are distributed across space for each time unit.\")\n",
    "print(f\"Total number of localities plotted: {len(filtered_localities)}\")\n",
    "print(f\"Range of occurrences per locality: {overall_min_occurrences} to {overall_max_occurrences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Based on exercises 4 and 5, what kind of observations about sampling\n",
    "can you make? Are there differences in sampling density over space and\n",
    "time? Compare some basic sampling properties between Africa, Asia and\n",
    "Europe, e.g. spatial coverage and average number of occurrences per\n",
    "locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continental boundaries (approximate)\n",
    "def assign_continent(row):\n",
    "    # Europe: roughly -10 to 30 longitude, 35 to 70 latitude\n",
    "    if -10 <= row['LONG'] <= 30 and 35 <= row['LAT'] <= 70:\n",
    "        return 'Europe'\n",
    "    # Africa: roughly -20 to 50 longitude, -40 to 35 latitude\n",
    "    elif -20 <= row['LONG'] <= 50 and row['LAT'] < 35:\n",
    "        return 'Africa'\n",
    "    # Asia: roughly 30 to 150 longitude, 0 to 70 latitude\n",
    "    elif row['LONG'] > 30 and 0 <= row['LAT'] <= 70:\n",
    "        return 'Asia'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Add continent information to the locality DataFrame\n",
    "locality_df['continent'] = locality_df.apply(assign_continent, axis=1)\n",
    "\n",
    "# Analyze sampling patterns by continent\n",
    "continent_counts = locality_df['continent'].value_counts()\n",
    "print(\"Number of localities by continent:\")\n",
    "print(continent_counts)\n",
    "\n",
    "# Calculate average occurrences per locality by continent\n",
    "avg_occurrences = locality_df.groupby('continent')['total_occurrences'].mean()\n",
    "print(\"\\nAverage occurrences per locality by continent:\")\n",
    "print(avg_occurrences)\n",
    "\n",
    "# Get spatial coverage statistics\n",
    "spatial_coverage = locality_df.groupby('continent').agg({\n",
    "    'LAT': ['std', 'min', 'max'],\n",
    "    'LONG': ['std', 'min', 'max'],\n",
    "    'total_occurrences': ['sum', 'mean', 'median']\n",
    "})\n",
    "print(\"\\nSpatial coverage and occurrence statistics by continent:\")\n",
    "print(spatial_coverage)\n",
    "\n",
    "# Time distribution analysis\n",
    "time_continent = pd.crosstab(locality_df['MN_Unit'], locality_df['continent'])\n",
    "print(\"\\nNumber of localities by time unit and continent:\")\n",
    "print(time_continent)\n",
    "\n",
    "# Visualize sampling density over time by continent\n",
    "valid_time_units = [u for u in valid_mn_units if u in time_continent.index]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for continent in ['Europe', 'Asia', 'Africa']:\n",
    "    if continent in time_continent.columns:\n",
    "        plt.plot(valid_time_units, \n",
    "                 time_continent.loc[valid_time_units, continent], \n",
    "                 marker='o', \n",
    "                 label=continent)\n",
    "plt.xlabel('Time Unit')\n",
    "plt.ylabel('Number of Localities')\n",
    "plt.title('Sampling Density Over Time by Continent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6. For each locality, look at a ten by ten degrees area (in\n",
    "latitude and longitude) centered around the locality. Record the total\n",
    "number of occurrences and total number of first occurrences found within\n",
    "that square in the time unit corresponding to the focal locality. Also,\n",
    "record the total number of occurrences within that square in the preceding\n",
    "time unit (relative to the focal locality). Record these numbers into the\n",
    "DataFrame that was created in exercise 5 (add new columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [],
   "source": [
    "# Define mapping to identify the preceding time unit\n",
    "mn_order_list = [\n",
    "    \"pre-MN\", \"MN1\", \"MN2\", \"MN3\", \"MN4\", \"MN5\", \"MN6\", \"MN7-8\", \n",
    "    \"MN9\", \"MN10\", \"MN11\", \"MN12\", \"MN13\", \"MN14\", \"MN15\", \n",
    "    \"MN16\", \"MN17\", \"MQ18\", \"MQ19\", \"post-MN\"\n",
    "]\n",
    "\n",
    "# Create a dictionary that maps each time unit to its preceding time unit\n",
    "preceding_time_unit = {mn_order_list[i]: mn_order_list[i-1] for i in range(1, len(mn_order_list))}\n",
    "preceding_time_unit[\"pre-MN\"] = None  # No preceding time unit for pre-MN\n",
    "\n",
    "# Initialize new columns in locality_df\n",
    "locality_df['square_total_occurrences'] = 0\n",
    "locality_df['square_first_occurrences'] = 0\n",
    "locality_df['preceding_square_total_occurrences'] = 0\n",
    "\n",
    "# For each locality, analyze the 10x10 degree area\n",
    "for idx, locality in locality_df.iterrows():\n",
    "    # Define the boundaries of the 10x10 degree square centered at the locality\n",
    "    min_lat = locality['LAT'] - 5\n",
    "    max_lat = locality['LAT'] + 5\n",
    "    min_long = locality['LONG'] - 5\n",
    "    max_long = locality['LONG'] + 5\n",
    "    \n",
    "    # Current time unit data\n",
    "    current_time = locality['MN_Unit']\n",
    "    \n",
    "    # Find all localities in that square with the same time unit\n",
    "    square_mask = (\n",
    "        (locality_df['LAT'] >= min_lat) & \n",
    "        (locality_df['LAT'] <= max_lat) & \n",
    "        (locality_df['LONG'] >= min_long) & \n",
    "        (locality_df['LONG'] <= max_long) & \n",
    "        (locality_df['MN_Unit'] == current_time)\n",
    "    )\n",
    "    square_localities = locality_df[square_mask]\n",
    "    \n",
    "    # Count total occurrences in the square for the current time unit\n",
    "    locality_df.at[idx, 'square_total_occurrences'] = square_localities['total_occurrences'].sum()\n",
    "    \n",
    "    # Count first occurrences in the square for the current time unit\n",
    "    locality_df.at[idx, 'square_first_occurrences'] = square_localities['first_occurrences'].sum()\n",
    "    \n",
    "    # Get preceding time unit\n",
    "    preceding_time = preceding_time_unit.get(current_time)\n",
    "    \n",
    "    if preceding_time:\n",
    "        # Find all localities in that square from the preceding time unit\n",
    "        preceding_square_mask = (\n",
    "            (locality_df['LAT'] >= min_lat) & \n",
    "            (locality_df['LAT'] <= max_lat) & \n",
    "            (locality_df['LONG'] >= min_long) & \n",
    "            (locality_df['LONG'] <= max_long) & \n",
    "            (locality_df['MN_Unit'] == preceding_time)\n",
    "        )\n",
    "        preceding_square_localities = locality_df[preceding_square_mask]\n",
    "        \n",
    "        # Count total occurrences in the square for the preceding time unit\n",
    "        locality_df.at[idx, 'preceding_square_total_occurrences'] = preceding_square_localities['total_occurrences'].sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Locality DataFrame with new square analysis columns:\")\n",
    "print(locality_df[['LIDNUM', 'LAT', 'LONG', 'MN_Unit', \n",
    "                   'total_occurrences', 'first_occurrences',\n",
    "                   'square_total_occurrences', 'square_first_occurrences',\n",
    "                   'preceding_square_total_occurrences']].head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics for the new columns:\")\n",
    "print(locality_df[['square_total_occurrences', 'square_first_occurrences', \n",
    "                  'preceding_square_total_occurrences']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7. a) Create the regression data set. Only use localities within\n",
    "the co-ordinates -25<LONG<40 and LAT>35 and time unit within MN2-\n",
    "MQ19 (why not include MN1?). Create an m × 2 array, where m is the\n",
    "total number of occurrences in all the localities. Each row in the array\n",
    "represents one occurrence. For each occurrence, fill in to the first column\n",
    "of the array the number of occurrences in the focal area in the previous\n",
    "time unit (calculated in exercise 6). For the second column, fill in 1 for a\n",
    "first occurrence and 0 for other occurrences.\n",
    "b) Perform logistic regression.\n",
    "c) Plot regression curve and 95%-confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a: Create the regression dataset\n",
    "# Filter localities based on geographic and temporal constraints\n",
    "filtered_localities = locality_df[\n",
    "    (locality_df['LONG'] > -25) & (locality_df['LONG'] < 40) &\n",
    "    (locality_df['LAT'] > 35) &\n",
    "    (locality_df['MN_Unit'].isin(mn_order_list[2:19]))  # MN2-MQ19\n",
    "].copy()\n",
    "\n",
    "print(f\"Number of localities after filtering: {len(filtered_localities)}\")\n",
    "\n",
    "# Create regression dataset - one row per occurrence\n",
    "regression_data = []\n",
    "\n",
    "for _, locality in filtered_localities.iterrows():\n",
    "    # Get the total number of occurrences and first occurrences for this locality\n",
    "    total_occurrences = int(locality['total_occurrences'])\n",
    "    first_occurrences = int(locality['first_occurrences'])\n",
    "    prev_occurrences = locality['preceding_square_total_occurrences']\n",
    "    \n",
    "    # Add rows for first occurrences (label = 1)\n",
    "    for _ in range(first_occurrences):\n",
    "        regression_data.append([prev_occurrences, 1])\n",
    "    \n",
    "    # Add rows for other occurrences (label = 0)\n",
    "    for _ in range(total_occurrences - first_occurrences):\n",
    "        regression_data.append([prev_occurrences, 0])\n",
    "\n",
    "# Convert to numpy array\n",
    "regression_array = np.array(regression_data)\n",
    "\n",
    "print(f\"Total occurrences in regression dataset: {len(regression_array)}\")\n",
    "print(f\"Number of first occurrences: {sum(regression_array[:, 1])}\")\n",
    "print(f\"First few rows of regression data:\")\n",
    "print(regression_array[:5])\n",
    "\n",
    "# Part b: Perform logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reshape X to be a 2D array\n",
    "X = regression_array[:, 0].reshape(-1, 1)\n",
    "y = regression_array[:, 1]\n",
    "\n",
    "# Fit the model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
    "print(f\"Coefficient: {model.coef_[0][0]:.4f}\")\n",
    "\n",
    "# Part c: Plot regression curve with 95% confidence intervals\n",
    "from scipy import stats\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_range = np.linspace(0, max(X) * 1.1, 1000).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred = model.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Calculate standard error for confidence intervals\n",
    "# Using the formula for the standard error of the log odds\n",
    "n_samples = len(X)\n",
    "p = y_pred\n",
    "se = np.sqrt(p * (1 - p) / n_samples)\n",
    "\n",
    "# Calculate confidence intervals (on the probability scale)\n",
    "z = stats.norm.ppf(0.975)  # 95% CI\n",
    "lower_ci = p - z * se\n",
    "upper_ci = p + z * se\n",
    "\n",
    "# Ensure bounds are between 0 and 1\n",
    "lower_ci = np.maximum(0, lower_ci)\n",
    "upper_ci = np.minimum(1, upper_ci)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the confidence interval as a shaded region\n",
    "plt.fill_between(x_range.flatten(), lower_ci, upper_ci, color='gray', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Plot the regression curve\n",
    "plt.plot(x_range, y_pred, 'r-', label='Predicted probability')\n",
    "\n",
    "# Add scatter plot of the data\n",
    "plt.scatter(X, y, color='blue', alpha=0.1, label='Data points')\n",
    "\n",
    "# Add a title and labels\n",
    "plt.title('Logistic Regression: Probability of First Occurrence vs. Previous Time Unit Occurrences')\n",
    "plt.xlabel('Number of Occurrences in Previous Time Unit')\n",
    "plt.ylabel('Probability of First Occurrence')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print model performance metrics\n",
    "y_pred_class = model.predict(X)\n",
    "print(\"\\nModel Performance:\")\n",
    "print(classification_report(y, y_pred_class))\n",
    "\n",
    "# Why not include MN1?\n",
    "print(\"\\nWhy not include MN1?\")\n",
    "print(\"MN1 is likely excluded because it's the earliest time unit in the series.\")\n",
    "print(\"Since we're looking at occurrences in the 'previous time unit', MN1 would have\")\n",
    "print(\"no previous time unit in this dataset (pre-MN might be too distant or sparse).\")\n",
    "print(\"This would create issues with the regression analysis as those data points\")\n",
    "print(\"would have missing or unreliable values for the predictor variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8. For each European locality, calculate the expected propor\u0002tion of first occurrences in the focal area surrounding the locality using\n",
    "the logistic regression calculated in exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9. For each European locality, calculate the probability of ob\u0002serving as many or more first occurrences in the focal area than what is\n",
    "actually found. Assume that occurrences are binomially distributed to\n",
    "“first occurrences” and “not first occurrences”, so that the probability of\n",
    "a given occurrence to be a first occurrence is equal to the expected pro\u0002portion of first occurrences in the focal area. You may use, for example,\n",
    "the scipy.stats.binom library (https://docs.scipy.org/doc/scipy-0.14.0/\n",
    "reference/generated/scipy.stats.binom.html) for the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for European localities\n",
    "european_localities = locality_df[locality_df['continent'] == 'Europe'].copy()\n",
    "\n",
    "# Ensure we're only using localities that match the criteria from Exercise 7a\n",
    "european_localities = european_localities[\n",
    "    (european_localities['LONG'] > -25) & (european_localities['LONG'] < 40) &\n",
    "    (european_localities['LAT'] > 35) &\n",
    "    (european_localities['MN_Unit'].isin(mn_order_list[2:19]))  # MN2-MQ19\n",
    "]\n",
    "\n",
    "# Calculate expected proportion of first occurrences using the logistic regression model\n",
    "european_localities['expected_proportion'] = model.predict_proba(\n",
    "    european_localities['preceding_square_total_occurrences'].values.reshape(-1, 1)# Filter for European localities\n",
    "european_localities = locality_df[locality_df['continent'] == 'Europe'].copy()\n",
    "\n",
    "# Ensure we're only using localities that match the criteria from Exercise 7a\n",
    "european_localities = european_localities[\n",
    "    (european_localities['LONG'] > -25) & (european_localities['LONG'] < 40) &\n",
    "    (european_localities['LAT'] > 35) &\n",
    "    (european_localities['MN_Unit'].isin(mn_order_list[2:19]))  # MN2-MQ19\n",
    "]\n",
    "\n",
    "# Calculate expected proportion of first occurrences using the logistic regression model\n",
    "european_localities['expected_proportion'] = model.predict_proba(\n",
    "    european_localities['preceding_square_total_occurrences'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 10. For each time unit, plot localities on a map covering the\n",
    "coordinates defined in exercise 7a and indicate their significance level with\n",
    "a sliding color scheme. Highlight localities that have p-value less than\n",
    "0.05 (i.e. probability of observations is less than 0.05). Describe briefly\n",
    "the overall patterns that you observe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
